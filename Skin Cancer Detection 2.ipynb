{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Cancer Detection 2 - bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "# from glob import glob\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 254, 254\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/valid'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 150\n",
    "epochs =100\n",
    "batch_size = 15\n",
    "\n",
    "def load_dataset(path, categories_count):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), categories_count)\n",
    "    return files, targets\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG19(\n",
    "        include_top=False, \n",
    "        weights='imagenet')\n",
    "\n",
    "#     generator = datagen.flow_from_directory(\n",
    "#         train_data_dir,\n",
    "#         target_size=(img_width, img_height),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode=\"categorical\",\n",
    "#         shuffle=False)\n",
    "    \n",
    "#     bottleneck_features_train = model.predict_generator(\n",
    "#         generator, nb_train_samples // batch_size)\n",
    "    \n",
    "#     np.save(open('bottleneck_features_train2.npy', 'wb'),\n",
    "#             bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    print(nb_validation_samples // batch_size)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_validation2.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train2.npy', 'rb'))\n",
    "    train_labels = load_dataset(train_data_dir, 3)[1]\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation2.npy', 'rb'))\n",
    "    validation_labels = load_dataset(validation_data_dir, 3)[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "    \n",
    "def train_top_model2():\n",
    "    train_data = np.load(open('bottleneck_features_train2.npy', 'rb'))\n",
    "    train_labels = load_dataset(train_data_dir, 3)[1]\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation2.npy', 'rb'))\n",
    "    validation_labels = load_dataset(validation_data_dir, 3)[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(2,2), strides=(1,1), padding='same', input_shape=(7, 7, 512), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(filters=16, kernel_size=(2,2), strides=(1,1), padding='same', activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(filters=8, kernel_size=(2,2), strides=(1,1), padding='same', activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GlobalAveragePooling2D(data_format=\"channels_last\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 32)          65568     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 16)          2064      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 8)           520       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 68,179\n",
      "Trainable params: 68,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.9046 - acc: 0.6745 - val_loss: 1.0358 - val_acc: 0.5200\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8861 - acc: 0.6855 - val_loss: 1.0309 - val_acc: 0.5200\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8824 - acc: 0.6860 - val_loss: 1.0609 - val_acc: 0.5200\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8729 - acc: 0.6860 - val_loss: 1.0358 - val_acc: 0.5200\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8718 - acc: 0.6860 - val_loss: 1.0423 - val_acc: 0.5200\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.8707 - acc: 0.6860 - val_loss: 1.0346 - val_acc: 0.5200\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8548 - acc: 0.6860 - val_loss: 1.0333 - val_acc: 0.5200\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8535 - acc: 0.6860 - val_loss: 1.0414 - val_acc: 0.5200\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8515 - acc: 0.6860 - val_loss: 1.0523 - val_acc: 0.5200\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8553 - acc: 0.6860 - val_loss: 1.0527 - val_acc: 0.5200\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.8581 - acc: 0.6860 - val_loss: 1.0724 - val_acc: 0.5200\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.8565 - acc: 0.6860 - val_loss: 1.0449 - val_acc: 0.5200\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8380 - acc: 0.6860 - val_loss: 1.0431 - val_acc: 0.5200\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8461 - acc: 0.6860 - val_loss: 1.0512 - val_acc: 0.5200\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8467 - acc: 0.6860 - val_loss: 1.0582 - val_acc: 0.5200\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8437 - acc: 0.6860 - val_loss: 1.0772 - val_acc: 0.5200\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8407 - acc: 0.6860 - val_loss: 1.0632 - val_acc: 0.5200\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8462 - acc: 0.6860 - val_loss: 1.0383 - val_acc: 0.5200\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8336 - acc: 0.6860 - val_loss: 1.0574 - val_acc: 0.5200\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8377 - acc: 0.6860 - val_loss: 1.0725 - val_acc: 0.5200\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8393 - acc: 0.6860 - val_loss: 1.0833 - val_acc: 0.5200\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.8392 - acc: 0.6860 - val_loss: 1.0612 - val_acc: 0.5200\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8383 - acc: 0.6860 - val_loss: 1.0663 - val_acc: 0.5200\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 1s 447us/step - loss: 0.8333 - acc: 0.6860 - val_loss: 1.0740 - val_acc: 0.5200\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 1s 460us/step - loss: 0.8311 - acc: 0.6860 - val_loss: 1.0730 - val_acc: 0.5200\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 1s 461us/step - loss: 0.8462 - acc: 0.6860 - val_loss: 1.0721 - val_acc: 0.5200\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.8350 - acc: 0.6860 - val_loss: 1.0763 - val_acc: 0.5200\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.8338 - acc: 0.6860 - val_loss: 1.0808 - val_acc: 0.5200\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.8331 - acc: 0.6860 - val_loss: 1.1301 - val_acc: 0.5200\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8364 - acc: 0.6860 - val_loss: 1.0624 - val_acc: 0.5200\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8325 - acc: 0.6860 - val_loss: 1.0693 - val_acc: 0.5200\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.8272 - acc: 0.6860 - val_loss: 1.0939 - val_acc: 0.5200\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.8299 - acc: 0.6860 - val_loss: 1.1454 - val_acc: 0.5200\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8308 - acc: 0.6860 - val_loss: 1.0726 - val_acc: 0.5200\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.8246 - acc: 0.6860 - val_loss: 1.1070 - val_acc: 0.5200\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8272 - acc: 0.6860 - val_loss: 1.1085 - val_acc: 0.5200\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.8267 - acc: 0.6860 - val_loss: 1.0962 - val_acc: 0.5200\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8203 - acc: 0.6860 - val_loss: 1.0883 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8257 - acc: 0.6860 - val_loss: 1.1158 - val_acc: 0.5200\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8259 - acc: 0.6860 - val_loss: 1.1192 - val_acc: 0.5200\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.8228 - acc: 0.6860 - val_loss: 1.2149 - val_acc: 0.5200\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.8286 - acc: 0.6860 - val_loss: 1.1485 - val_acc: 0.5200\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8222 - acc: 0.6860 - val_loss: 1.1822 - val_acc: 0.5200\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8143 - acc: 0.6860 - val_loss: 1.1610 - val_acc: 0.5200\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8127 - acc: 0.6860 - val_loss: 1.1328 - val_acc: 0.5200\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8153 - acc: 0.6860 - val_loss: 1.1814 - val_acc: 0.5200\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8160 - acc: 0.6860 - val_loss: 1.1785 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8134 - acc: 0.6860 - val_loss: 1.2135 - val_acc: 0.5200\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.8169 - acc: 0.6860 - val_loss: 1.1415 - val_acc: 0.5200\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.8141 - acc: 0.6860 - val_loss: 1.2226 - val_acc: 0.5200\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.8063 - acc: 0.6860 - val_loss: 1.1230 - val_acc: 0.5200\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8133 - acc: 0.6860 - val_loss: 1.6151 - val_acc: 0.5200\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8102 - acc: 0.6860 - val_loss: 1.1360 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.8114 - acc: 0.6860 - val_loss: 1.1268 - val_acc: 0.5200\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 1s 463us/step - loss: 0.8142 - acc: 0.6860 - val_loss: 1.3430 - val_acc: 0.5200\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.8121 - acc: 0.6860 - val_loss: 1.1630 - val_acc: 0.5200\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.8024 - acc: 0.6860 - val_loss: 1.1842 - val_acc: 0.5200\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.8051 - acc: 0.6860 - val_loss: 1.2114 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.8002 - acc: 0.6860 - val_loss: 1.1826 - val_acc: 0.5200\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.8016 - acc: 0.6860 - val_loss: 1.2839 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.8028 - acc: 0.6860 - val_loss: 1.6243 - val_acc: 0.5200\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.8026 - acc: 0.6860 - val_loss: 1.2842 - val_acc: 0.5200\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.8016 - acc: 0.6860 - val_loss: 1.2113 - val_acc: 0.5200\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.8006 - acc: 0.6860 - val_loss: 1.2074 - val_acc: 0.5200\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7964 - acc: 0.6860 - val_loss: 1.2713 - val_acc: 0.5200\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.7986 - acc: 0.6860 - val_loss: 1.1774 - val_acc: 0.5200\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.8007 - acc: 0.6860 - val_loss: 1.2644 - val_acc: 0.5200\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.7929 - acc: 0.6860 - val_loss: 1.2253 - val_acc: 0.5200\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.8054 - acc: 0.6860 - val_loss: 1.3253 - val_acc: 0.5200\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.7935 - acc: 0.6860 - val_loss: 1.3574 - val_acc: 0.5200\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.7936 - acc: 0.6860 - val_loss: 1.2241 - val_acc: 0.5200\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.7888 - acc: 0.6860 - val_loss: 1.3164 - val_acc: 0.5200\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.7946 - acc: 0.6860 - val_loss: 1.2822 - val_acc: 0.5200\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.7972 - acc: 0.6860 - val_loss: 1.5613 - val_acc: 0.5200\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.7896 - acc: 0.6860 - val_loss: 1.3153 - val_acc: 0.5200\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.7836 - acc: 0.6860 - val_loss: 1.3028 - val_acc: 0.5200\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.7876 - acc: 0.6860 - val_loss: 1.4042 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7794 - acc: 0.6860 - val_loss: 1.3815 - val_acc: 0.5200\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.7846 - acc: 0.6860 - val_loss: 1.4223 - val_acc: 0.5200\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.7772 - acc: 0.6860 - val_loss: 1.3418 - val_acc: 0.5200\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.7848 - acc: 0.6860 - val_loss: 1.3891 - val_acc: 0.5200\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.7793 - acc: 0.6860 - val_loss: 1.3585 - val_acc: 0.5200\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.7957 - acc: 0.6860 - val_loss: 1.2449 - val_acc: 0.5200\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.7782 - acc: 0.6860 - val_loss: 1.3050 - val_acc: 0.5200\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.7933 - acc: 0.6860 - val_loss: 1.4137 - val_acc: 0.5200\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.7824 - acc: 0.6860 - val_loss: 1.3212 - val_acc: 0.5200\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7720 - acc: 0.6860 - val_loss: 1.3492 - val_acc: 0.5200\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.7661 - acc: 0.6860 - val_loss: 1.4361 - val_acc: 0.5200\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.7764 - acc: 0.6860 - val_loss: 1.5143 - val_acc: 0.5200\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.7757 - acc: 0.6860 - val_loss: 1.4206 - val_acc: 0.5200\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7713 - acc: 0.6860 - val_loss: 1.5918 - val_acc: 0.5200\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.7728 - acc: 0.6860 - val_loss: 1.2992 - val_acc: 0.5200\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.7736 - acc: 0.6860 - val_loss: 1.7026 - val_acc: 0.5200\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.7721 - acc: 0.6860 - val_loss: 1.4366 - val_acc: 0.5200\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.7712 - acc: 0.6860 - val_loss: 1.5255 - val_acc: 0.5200\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.7684 - acc: 0.6860 - val_loss: 1.7710 - val_acc: 0.5200\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7697 - acc: 0.6860 - val_loss: 1.4093 - val_acc: 0.5200\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.7767 - acc: 0.6860 - val_loss: 1.6129 - val_acc: 0.5200\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.7692 - acc: 0.6860 - val_loss: 1.6246 - val_acc: 0.5200\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.7710 - acc: 0.6860 - val_loss: 1.3981 - val_acc: 0.5200\n"
     ]
    }
   ],
   "source": [
    "train_top_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     train_data = np.load(open('bottleneck_features_train2.npy', 'rb'))\n",
    "#     train_labels = load_dataset(train_data_dir, 3)[1]\n",
    "\n",
    "#     validation_data = np.load(open('bottleneck_features_validation2.npy', 'rb'))\n",
    "#     validation_labels = load_dataset(validation_data_dir, 3)[1]\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('bottleneck_features_train2.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 7, 7, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 2s 852us/step - loss: 1.2203 - acc: 0.2410 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 1s 627us/step - loss: 1.1833 - acc: 0.2290 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 1.1520 - acc: 0.2110 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 1.1023 - acc: 0.1925 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 1s 630us/step - loss: 1.1204 - acc: 0.2095 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 1s 629us/step - loss: 1.1382 - acc: 0.1990 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 1.1637 - acc: 0.2025 - val_loss: 1.0970 - val_acc: 0.2000\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 1s 644us/step - loss: 1.1526 - acc: 0.2560 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 1s 625us/step - loss: 1.1476 - acc: 0.2185 - val_loss: 1.0986 - val_acc: 0.2000\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 1s 625us/step - loss: 1.1673 - acc: 0.2310 - val_loss: 1.0957 - val_acc: 0.2000\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 1.1682 - acc: 0.3065 - val_loss: 1.1204 - val_acc: 0.2000\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 1s 625us/step - loss: 1.1670 - acc: 0.3050 - val_loss: 1.0948 - val_acc: 0.2000\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 1s 629us/step - loss: 1.1181 - acc: 0.2505 - val_loss: 1.1214 - val_acc: 0.2000\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 1s 626us/step - loss: 1.1773 - acc: 0.2830 - val_loss: 1.1018 - val_acc: 0.2000\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 1s 630us/step - loss: 1.1454 - acc: 0.2975 - val_loss: 1.1578 - val_acc: 0.2067\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 1s 629us/step - loss: 1.1746 - acc: 0.3625 - val_loss: 1.1257 - val_acc: 0.2800\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 1s 627us/step - loss: 1.1382 - acc: 0.4465 - val_loss: 1.2796 - val_acc: 0.5200\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 1s 625us/step - loss: 1.0177 - acc: 0.6570 - val_loss: 1.1792 - val_acc: 0.5200\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.9092 - acc: 0.6855 - val_loss: 1.1267 - val_acc: 0.5200\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 1s 626us/step - loss: 0.8927 - acc: 0.6860 - val_loss: 1.0988 - val_acc: 0.5200\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 1s 627us/step - loss: 0.8905 - acc: 0.6860 - val_loss: 1.2230 - val_acc: 0.5200\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.8698 - acc: 0.6860 - val_loss: 1.1767 - val_acc: 0.5200\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 1s 627us/step - loss: 0.8993 - acc: 0.6855 - val_loss: 1.1774 - val_acc: 0.5200\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 1s 626us/step - loss: 0.8683 - acc: 0.6855 - val_loss: 1.1103 - val_acc: 0.5200\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 1s 629us/step - loss: 0.8600 - acc: 0.6865 - val_loss: 1.6450 - val_acc: 0.5200\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.8689 - acc: 0.6865 - val_loss: 1.3123 - val_acc: 0.5200\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 1s 633us/step - loss: 0.8793 - acc: 0.6860 - val_loss: 1.1704 - val_acc: 0.5200\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 1s 632us/step - loss: 0.8499 - acc: 0.6860 - val_loss: 1.1217 - val_acc: 0.5200\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 1s 628us/step - loss: 0.8404 - acc: 0.6860 - val_loss: 1.2608 - val_acc: 0.5200\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 1s 632us/step - loss: 0.8413 - acc: 0.6860 - val_loss: 1.1666 - val_acc: 0.5200\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.8518 - acc: 0.6860 - val_loss: 1.1923 - val_acc: 0.5200\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 1s 633us/step - loss: 0.8319 - acc: 0.6865 - val_loss: 1.1885 - val_acc: 0.5200\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.8346 - acc: 0.6850 - val_loss: 1.1946 - val_acc: 0.5200\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.8148 - acc: 0.6875 - val_loss: 1.2598 - val_acc: 0.5200\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 1s 647us/step - loss: 0.8084 - acc: 0.6845 - val_loss: 1.4750 - val_acc: 0.5200\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.8042 - acc: 0.6870 - val_loss: 1.4089 - val_acc: 0.5200\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.8053 - acc: 0.6860 - val_loss: 1.3541 - val_acc: 0.5200\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.8077 - acc: 0.6855 - val_loss: 1.5268 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.8014 - acc: 0.6855 - val_loss: 1.3010 - val_acc: 0.5200\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 1s 635us/step - loss: 0.7954 - acc: 0.6870 - val_loss: 1.5373 - val_acc: 0.5200\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 1s 644us/step - loss: 0.7999 - acc: 0.6860 - val_loss: 1.9318 - val_acc: 0.5200\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 1s 652us/step - loss: 0.7828 - acc: 0.6870 - val_loss: 1.5089 - val_acc: 0.5200\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.7798 - acc: 0.6865 - val_loss: 1.3651 - val_acc: 0.5200\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 1s 659us/step - loss: 0.7839 - acc: 0.6870 - val_loss: 1.8771 - val_acc: 0.5200\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 1s 649us/step - loss: 0.7934 - acc: 0.6865 - val_loss: 1.8876 - val_acc: 0.5200\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 1s 646us/step - loss: 0.7865 - acc: 0.6860 - val_loss: 2.1997 - val_acc: 0.5200\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 1s 652us/step - loss: 0.7767 - acc: 0.6875 - val_loss: 1.5443 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 1s 645us/step - loss: 0.7994 - acc: 0.6870 - val_loss: 1.5523 - val_acc: 0.5200\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.7888 - acc: 0.6880 - val_loss: 1.8720 - val_acc: 0.5200\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 1s 650us/step - loss: 0.7767 - acc: 0.6870 - val_loss: 1.8949 - val_acc: 0.5200\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.7584 - acc: 0.6870 - val_loss: 1.4720 - val_acc: 0.5200\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 1s 645us/step - loss: 0.7750 - acc: 0.6875 - val_loss: 1.7500 - val_acc: 0.5200\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 1s 646us/step - loss: 0.7725 - acc: 0.6885 - val_loss: 1.5941 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7675 - acc: 0.6865 - val_loss: 1.8245 - val_acc: 0.5200\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.7733 - acc: 0.6905 - val_loss: 1.8223 - val_acc: 0.5200\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.7649 - acc: 0.6885 - val_loss: 1.5176 - val_acc: 0.5267\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 1s 642us/step - loss: 0.7816 - acc: 0.6895 - val_loss: 2.4151 - val_acc: 0.5200\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7817 - acc: 0.6885 - val_loss: 1.8265 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.7767 - acc: 0.6890 - val_loss: 2.0081 - val_acc: 0.5200\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.7748 - acc: 0.6900 - val_loss: 2.2909 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 1s 639us/step - loss: 0.7921 - acc: 0.6900 - val_loss: 1.9097 - val_acc: 0.5200\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 1s 658us/step - loss: 0.7554 - acc: 0.6870 - val_loss: 1.9429 - val_acc: 0.5200\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.7557 - acc: 0.6895 - val_loss: 1.6883 - val_acc: 0.5200\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 1s 647us/step - loss: 0.7521 - acc: 0.6940 - val_loss: 1.7082 - val_acc: 0.5200\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 1s 634us/step - loss: 0.7733 - acc: 0.6920 - val_loss: 1.9978 - val_acc: 0.5200\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7638 - acc: 0.6910 - val_loss: 1.8493 - val_acc: 0.5200\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 1s 650us/step - loss: 0.7498 - acc: 0.6890 - val_loss: 1.7562 - val_acc: 0.5200\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 1s 635us/step - loss: 0.7542 - acc: 0.6940 - val_loss: 1.9734 - val_acc: 0.5200\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 1s 642us/step - loss: 0.7405 - acc: 0.6925 - val_loss: 2.0586 - val_acc: 0.5200\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.7621 - acc: 0.6895 - val_loss: 2.1455 - val_acc: 0.5200\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 1s 635us/step - loss: 0.7638 - acc: 0.6925 - val_loss: 1.7655 - val_acc: 0.5200\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 1s 639us/step - loss: 0.7458 - acc: 0.6920 - val_loss: 1.7640 - val_acc: 0.5200\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7539 - acc: 0.6900 - val_loss: 1.7267 - val_acc: 0.5200\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7640 - acc: 0.6915 - val_loss: 2.0680 - val_acc: 0.5200\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7734 - acc: 0.6945 - val_loss: 1.7677 - val_acc: 0.5200\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.7502 - acc: 0.6955 - val_loss: 2.5772 - val_acc: 0.5200\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.7515 - acc: 0.6955 - val_loss: 1.9066 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 1s 646us/step - loss: 0.7663 - acc: 0.6965 - val_loss: 1.9555 - val_acc: 0.5133\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 1s 644us/step - loss: 0.7562 - acc: 0.6990 - val_loss: 2.0180 - val_acc: 0.5200\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.7789 - acc: 0.6985 - val_loss: 1.7682 - val_acc: 0.5200\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 1s 649us/step - loss: 0.7398 - acc: 0.6955 - val_loss: 2.4926 - val_acc: 0.5133\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 1s 638us/step - loss: 0.7516 - acc: 0.7000 - val_loss: 2.4765 - val_acc: 0.5133\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.7691 - acc: 0.6980 - val_loss: 2.0872 - val_acc: 0.5133\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.7608 - acc: 0.6980 - val_loss: 2.0387 - val_acc: 0.5133\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.7618 - acc: 0.6980 - val_loss: 2.4724 - val_acc: 0.4933\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 1s 646us/step - loss: 0.7629 - acc: 0.7010 - val_loss: 2.5547 - val_acc: 0.5133\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.7761 - acc: 0.7015 - val_loss: 2.7250 - val_acc: 0.5133\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 1s 639us/step - loss: 0.7628 - acc: 0.7020 - val_loss: 2.1244 - val_acc: 0.5200\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 1s 661us/step - loss: 0.7476 - acc: 0.6965 - val_loss: 2.0871 - val_acc: 0.5133\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 1s 668us/step - loss: 0.7582 - acc: 0.6965 - val_loss: 2.3289 - val_acc: 0.5133\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 1s 661us/step - loss: 0.7565 - acc: 0.6955 - val_loss: 2.3671 - val_acc: 0.5133\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.7420 - acc: 0.7010 - val_loss: 2.3398 - val_acc: 0.5067\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 1s 636us/step - loss: 0.7421 - acc: 0.7020 - val_loss: 2.3672 - val_acc: 0.5133\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 1s 641us/step - loss: 0.7484 - acc: 0.6950 - val_loss: 2.2870 - val_acc: 0.5067\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 1s 635us/step - loss: 0.7599 - acc: 0.7015 - val_loss: 2.0734 - val_acc: 0.5133\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.7507 - acc: 0.7005 - val_loss: 2.4579 - val_acc: 0.5133\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 1s 639us/step - loss: 0.7570 - acc: 0.7025 - val_loss: 2.3992 - val_acc: 0.5133\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 1s 640us/step - loss: 0.7636 - acc: 0.6975 - val_loss: 2.7463 - val_acc: 0.5200\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 0.7489 - acc: 0.7025 - val_loss: 2.3980 - val_acc: 0.5200\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 1s 643us/step - loss: 0.7403 - acc: 0.6985 - val_loss: 2.3287 - val_acc: 0.5067\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
